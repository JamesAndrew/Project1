\section{Summary} \label{summary}
Though, on average, K-Nearest Neighbor and ID3 performed better than the Bayes algorithms, true to the idea of no free lunch, all algorithms had at least one data set where they performed better than all the other algorithms (with Naive Bayes as an exception almost outperforming on the \textit{Breast Cancer} data set).
Naive Bayes and TAN performed similarly as both had the same best result (\textit{Breast-Cancer}) and worst results (\textit{Glass} and \textit{Iris}) and had similar trends across the remaining sets. 
As expected, TAN performer better than Naive Bayes on all data sets.
Results for K-Nearest Neighbor show that the algorithm is not necessarily dependent on the size of the data set but instead on the amount of spacial separation between the different classifications. 
Finally, the results from the ID3 algorithm show that of the earlier hypotheses were incorrect and the size of the data sets actually had little to do with the final classification results and instead, the number of attributes seems to have the most influence, with the best results coming from sets with the largest number of attributes per class. 