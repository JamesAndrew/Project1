VALUE ITERATION:
 - Using Bellman equation to calculate utility of being in any state (use equation from book)
 - Initial utility for all states (except finish state) is 0
 - Utilities are updated until equalibrium is reach (no utility changes from iteration i to iteration i+1)
 - Set epsilon to be 1x10^-6 or maybe 1x10^-8 ?
 - Discount factor of 0.5 to strike balance between notable distant reward and current additive reward
 - Talk about choice of utility for '#' squares. Example occurs in the catch block of the bellman equation
 
Q-LEARNING:
 - Remember to talk about each tunable parameter
 - Using an epsilon greedy policy for choosing what action to take
 - Q-Value table is contained as properties in each cell
 - Forcing initial trial runs to start at a state close to the finish line
 
RACECAR:
 - For diagonal movement, the car always travels the full row distance first, then full column distance (need 
   to update this to match Tyler's appraoch)
 