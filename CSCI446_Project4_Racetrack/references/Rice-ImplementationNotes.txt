VALUE ITERATION:
 - Using Bellman equation to calculate utility of being in any state (use equation from book)
 - Initial utility for all states (except finish state) is 0
 - Utilities are updated until equalibrium is reach (no utility changes from iteration i to iteration i+1)
 - Set epsilon to be 1x10^-6 or maybe 1x10^-8 ?
 - Discount factor of 0.5 to strike balance between notable distant reward and current additive reward
 - Talk about choice of utility for '#' squares. Example occurs in the catch block of the bellman equation
 
Q-LEARNING:
 - Remember to talk about each tunable parameter (lots to talk about)
 - Using an epsilon greedy policy for choosing what action to take
 - Q-Value table is contained as properties in each cell
 - Forcing initial trial runs to start at a state close to the finish line
 - Not using a frequency-action pair table
 - Am using epsilon-greedy function to handle greed vs. exploration
 - Talk about switching from exploration to race mode once convergence happens
 
RACECAR:
 - For diagonal movement, the car always travels the full row distance first, then full column distance (need 
   to update this to match Tyler's appraoch)
 
TODOs:
 - Update next state placements in both VI and Q-L to use Tyler's geometric appraoch
 - Enforce the randomized start index for Q-Learning doesn't start too close to the finish line
 - Reset on crash version for Q-Learning