Track tracks/Simple-track2.txt (r, c)= (7, 7)
#######
#FF####
#..####
#..####
#....S#
#....S#
#######

Beginning Q-Learning on track simple2
Tunable Parameters:
  epsilon halt threshold:  0.100000
  greedy action selection chance: 0.4000
  discount factor: 1.0000
  learning factor: 0.9000

Current generation: 0
  Initial State -- Location: [2,1], Velocity: [-1,0]
  === Current agent step t: 0 ===
#######
#FF####
#O.####
#..####
#....S#
#....S#
#######
    Finding random action for cell (2,1) with velocity (-1,0)
    Random acceleration action assigned to [1,0]
    Evaluating Q(s,a) <- Q(s,a) + alpha(R(s) + gamma*maxQ(s',a') - Q(s,a))...
      s' -- Location: [2,1], Velocity: [0,0]
      max a': [1,0]
      maxQ(s',a'): 0.000
      Q(s,a) before update: 0.000
      Q(s,a) after update: -0.900
    Moved agent to next state -- Location: [2,1], Velocity: [0,0]
  === Current agent step t: 1 ===
#######
#FF####
#O.####
#..####
#....S#
#....S#
#######
    Finding random action for cell (2,1) with velocity (0,0)
    Random acceleration action assigned to [0,-1]
    Evaluating Q(s,a) <- Q(s,a) + alpha(R(s) + gamma*maxQ(s',a') - Q(s,a))...
      s' -- Location: [2,1], Velocity: [0,0]
      max a': [0,-1]
      maxQ(s',a'): 0.000
      Q(s,a) before update: 0.000
      Q(s,a) after update: -0.900
    Moved agent to next state -- Location: [2,1], Velocity: [0,0]
  === Current agent step t: 2 ===
#######
#FF####
#O.####
#..####
#....S#
#....S#
#######
    Finding random action for cell (2,1) with velocity (0,0)
    Random acceleration action assigned to [-1,1]
    Evaluating Q(s,a) <- Q(s,a) + alpha(R(s) + gamma*maxQ(s',a') - Q(s,a))...
      s' -- Location: [1,1], Velocity: [0,0]
      max a': [1,0]
      maxQ(s',a'): 1.000
      Q(s,a) before update: 0.000
      Q(s,a) after update: 0.000
    Moved agent to next state -- Location: [1,1], Velocity: [0,0]
#######
#OF####
#..####
#..####
#....S#
#....S#
#######
    Reached end state. Ending current agent run.

Current generation: 1
  Picking initial state randomly.
  Initial State -- Location: [3,1], Velocity: [1,1]
  === Current agent step t: 0 ===
#######
#FF####
#..####
#O.####
#....S#
#....S#
#######
    Finding best action for cell (3,1) with velocity (1,1)
	.
	. (etc.)
	.

Current generation: 162
  Picking initial state randomly.
  Initial State -- Location: [4,3], Velocity: [0,-1]
  === Current agent step t: 0 ===
#######
#FF####
#..####
#..####
#..O.S#
#....S#
#######
    Finding random action for cell (4,3) with velocity (0,-1)
    Random acceleration action assigned to [-1,1]
    Evaluating Q(s,a) <- Q(s,a) + alpha(R(s) + gamma*maxQ(s',a') - Q(s,a))...
      s' -- Location: [4,3], Velocity: [0,0]
      max a': [0,-1]
      maxQ(s',a'): -2.000
      Q(s,a) before update: -3.000
      Q(s,a) after update: -3.000
    Moved agent to next state -- Location: [4,3], Velocity: [0,0]
  === Current agent step t: 1 ===
#######
#FF####
#..####
#..####
#..O.S#
#....S#
#######
    Finding best action for cell (4,3) with velocity (0,0)
    Best acceleration action found to be [0,-1]
    Evaluating Q(s,a) <- Q(s,a) + alpha(R(s) + gamma*maxQ(s',a') - Q(s,a))...
      s' -- Location: [4,2], Velocity: [0,-1]
      max a': [-1,0]
      maxQ(s',a'): -1.000
      Q(s,a) before update: -2.000
      Q(s,a) after update: -2.000
    Moved agent to next state -- Location: [4,2], Velocity: [0,-1]
  === Current agent step t: 2 ===
	.
	. (etc)
	.
    Reached end state. Ending current agent run.

Current generation: 163
  Picking initial state randomly.
  Initial State -- Location: [4,4], Velocity: [1,-1]
  === Current agent step t: 0 ===
#######
#FF####
#..####
#..####
#...OS#
#....S#
#######
    Finding random action for cell (4,4) with velocity (1,-1)
    Random acceleration action assigned to [-1,1]
    Evaluating Q(s,a) <- Q(s,a) + alpha(R(s) + gamma*maxQ(s',a') - Q(s,a))...
      s' -- Location: [4,4], Velocity: [0,0]
      max a': [0,-1]
      maxQ(s',a'): -2.000
      Q(s,a) before update: -3.000
      Q(s,a) after update: -3.000
    Moved agent to next state -- Location: [4,4], Velocity: [0,0]
  === Current agent step t: 1 ===
#######
#FF####
#..####
#..####
#...OS#
#....S#
#######
    Finding random action for cell (4,4) with velocity (0,0)
    Random acceleration action assigned to [0,-1]
    Evaluating Q(s,a) <- Q(s,a) + alpha(R(s) + gamma*maxQ(s',a') - Q(s,a))...
      s' -- Location: [4,3], Velocity: [0,-1]
      max a': [1,-1]
      maxQ(s',a'): -1.000
      Q(s,a) before update: -2.000
      Q(s,a) after update: -2.000
    Moved agent to next state -- Location: [4,3], Velocity: [0,-1]
  === Current agent step t: 2 ===
#######
#FF####
#..####
#..####
#..O.S#
#....S#
#######
    Finding random action for cell (4,3) with velocity (0,-1)
    Random acceleration action assigned to [-1,1]
    Evaluating Q(s,a) <- Q(s,a) + alpha(R(s) + gamma*maxQ(s',a') - Q(s,a))...
      s' -- Location: [4,3], Velocity: [0,0]
      max a': [0,-1]
      maxQ(s',a'): -2.000
      Q(s,a) before update: -3.000
      Q(s,a) after update: -3.000
    Moved agent to next state -- Location: [4,3], Velocity: [0,0]
  === Current agent step t: 3 ===
#######
#FF####
#..####
#..####
#..O.S#
#....S#
#######
    Finding best action for cell (4,3) with velocity (0,0)
    Best acceleration action found to be [0,-1]
    Evaluating Q(s,a) <- Q(s,a) + alpha(R(s) + gamma*maxQ(s',a') - Q(s,a))...
      s' -- Location: [4,2], Velocity: [0,-1]
      max a': [0,0]
      maxQ(s',a'): -1.000
      Q(s,a) before update: -2.000
      Q(s,a) after update: -2.000
    Moved agent to next state -- Location: [4,2], Velocity: [0,-1]
  === Current agent step t: 4 ===
#######
#FF####
#..####
#..####
#.O..S#
#....S#
#######
    Finding best action for cell (4,2) with velocity (0,-1)
    Best acceleration action found to be [-1,0]
    Evaluating Q(s,a) <- Q(s,a) + alpha(R(s) + gamma*maxQ(s',a') - Q(s,a))...
      s' -- Location: [3,1], Velocity: [-1,-1]
      max a': [-1,-1]
      maxQ(s',a'): 0.000
      Q(s,a) before update: -1.000
      Q(s,a) after update: -1.000
    Moved agent to next state -- Location: [3,1], Velocity: [-1,-1]
  === Current agent step t: 5 ===
#######
#FF####
#..####
#O.####
#....S#
#....S#
#######
    Finding random action for cell (3,1) with velocity (-1,-1)
    Random acceleration action assigned to [-1,-1]
    Evaluating Q(s,a) <- Q(s,a) + alpha(R(s) + gamma*maxQ(s',a') - Q(s,a))...
      s' -- Location: [1,1], Velocity: [0,0]
      max a': [-1,1]
      maxQ(s',a'): 1.000
      Q(s,a) before update: 0.000
      Q(s,a) after update: 0.000
    Moved agent to next state -- Location: [1,1], Velocity: [0,0]
#######
#OF####
#..####
#..####
#....S#
#....S#
#######
    Reached end state. Ending current agent run.

	
Exploration halting state reached. Running agent as a racer from the Start location
Current State -- Location: [5,5], Velocity: [0,0]
#######
#FF####
#..####
#..####
#....S#
#....O#
#######
Best Q(s,a) acceleration action to take: [-1,-1]
Moved agent to next state -- Location: [4,4], Velocity: [-1,-1]
#######
#FF####
#..####
#..####
#...OS#
#....S#
#######
Best Q(s,a) acceleration action to take: [1,-1]
Moved agent to next state -- Location: [4,2], Velocity: [0,-2]
#######
#FF####
#..####
#..####
#.O..S#
#....S#
#######
Best Q(s,a) acceleration action to take: [1,1]
Moved agent to next state -- Location: [5,1], Velocity: [1,-1]
#######
#FF####
#..####
#..####
#....S#
#O...S#
#######
Best Q(s,a) acceleration action to take: [0,0]
Moved agent to next state -- Location: [5,1], Velocity: [0,0]
#######
#FF####
#..####
#..####
#....S#
#O...S#
#######
Best Q(s,a) acceleration action to take: [-1,0]
Moved agent to next state -- Location: [4,1], Velocity: [-1,0]
#######
#FF####
#..####
#..####
#O...S#
#....S#
#######
Best Q(s,a) acceleration action to take: [0,1]
Moved agent to next state -- Location: [3,2], Velocity: [-1,1]
#######
#FF####
#..####
#.O####
#....S#
#....S#
#######
Best Q(s,a) acceleration action to take: [-1,-1]
Moved agent to next state -- Location: [1,2], Velocity: [0,0]
#######
#FO####
#..####
#..####
#....S#
#....S#
#######
Reached finish line. Ending Q-Learning simulation.

     === Displaying Results for QLearningStatistics ===
Average Training Iterations   Average Steps to Finish Race  
5.000                         6.000                         

Convergence rate for each discount paremeter setting:
0.20      0.40      0.60      0.80      1.00      
---------------------------------------------
NaN       NaN       NaN       NaN       5.00      

Average steps to finish race for each discount paremeter setting:
0.20      0.40      0.60      0.80      1.00      
---------------------------------------------
NaN       NaN       NaN       NaN       6.00      

Convergence rate for each learning paremeter setting:
0.50      0.70      0.80      0.90      1.00      
---------------------------------------------
NaN       NaN       NaN       5.00      5.00      

Average steps to finish race for each learning paremeter setting:
0.50      0.70      0.80      0.90      1.00      
---------------------------------------------
NaN       NaN       NaN       6.00      6.00      